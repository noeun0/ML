# 다항회귀

> 종속변수가 독립변수의 다항식으로 표시되는 경우의 회귀 분석. 2차, 3차 방정시기과 같은 다항식으로 표현되는 것을 다항 회귀라고 한다. 
>
> ![image-20210401114433525](C:\Users\rey\AppData\Roaming\Typora\typora-user-images\image-20210401114433525.png)
>
> 다항회귀는 선형회귀.  회귀에서 선형 비선형을 나누는 기준은 회귀 계수가 선형/ 비선형 인지에 따른 것. 독립변수는 무관.



- degree = 차수

  차수가 너무 커지면 새로운 데이터에 대한 예측력은 떨어짐 Overfitting 과적합 

  train에 대해선 고성능, test에 대해선 저성능



--------------------------------

## Ridge Regression

> 오차를 갖고 최적화를 하는데, 오차를 더 크게 만들어서 규제하기 위해 규제항을 넣어서 성능 지표를 올리는 방식으로 처리를 한다.

- 0에 가까운 값으로 바뀜





## Logistic 함수

> =시그모이드 함수 s자 모양을 띄고 있다.
>
> 선형 회귀 방식을 분류에 적용한 앍리즘. 시그모이드 함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정한 다는 것



- 시그모이드 함수는 x 값이 +,-로 아무리 커지거나 작아져도 y 값은 항상 0과 1사이의 값을 반환한다.
  - x 값이 증가할수록 1에 근사
  - x값이 감소 할수록 0에 근사

입력 특성은 정해져 있으므로 학습을 통해 최적의 가중치를 구해야 한다. 즉 최적화 한다는 것은 학습 데이터의 패턴을 가장 잘 찾는 모델을 만들기 위해 **가중치** = 파라미터 값을 구하는 것이다.



#### 손실함수

> 로지스틱의 전체 데이터 셋에 대한 손실함수는 아래 공식과 같다.(오차를 알려주는 함수)
>
> ![image-20210401154822938](C:\Users\rey\AppData\Roaming\Typora\typora-user-images\image-20210401154822938.png)
>
> p : 0~1 사이의 값, 

최적화란?

- 손실함수의 손실을 가장 적게하는 가중치 파라미터 값을 찾는 것

  ![image-20210401155436941](C:\Users\rey\AppData\Roaming\Typora\typora-user-images\image-20210401155436941.png)

  log(1) = 0

  log(0.5) = -0.69123123

  log(0.1) = -2.30

  log(0.00001) = -20.1231

  -> 로그 밑 값이 0에 가까워 질수록 기하급수적으로 빠르게  y값은 감소한다.



- 로그 손실함수는 최소값을 찾는 정규 방정식이 없기 때문에 경사 하강법 gradient desending을 사용해서 최적화 진행

  

#### 최적화 문제를 해결하는 방법

- 최소삾을 찾는 함수를 찾는다

  - 공식을 찾을 수 없는 경우가 있다.
  - feature와 sample 수가 많아 질 수록 계산량이 급증

- 경사 하강법 

  - 값을 조금씩 조정해 나가면서 최소값을 찾는다
  - 미분을 이용한다. (미분 : 변화율을 구하는 것.)

  - 경사를 줄일 수 있는 방향으로 이동하여 최저 점을 찾는다
  - 학습률 : 얼마만큼 음직일 것인가? - 우리가 지정

---------------------------------

#### 경사 하강법

```
미분이 필요한 이유? 방향 찾기 위해
```



---------------

### Logistic Regression 주요 하이퍼파라미터



- pe

- c : 규제 강도 - 작을 수록 규제가 강하다

max_iter : 최대 반복 횟수

 에러가 발생하면 max_iter값을 증가시키자

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accurancy_score,classification_report

lr = logisticRegression(C=0.1, random_State=1)
lr.fix(X_train_scaler,y_train)

pred_train
```





